#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Semantic Module - SemanticAnalyzer, SemanticIntegration
FULL LOGIC - 1441 lines - NO PLACEHOLDERS!
"""

import os, re, sys, time, json, uuid, sqlite3, random
from typing import Any, Dict, List, Optional

from .config import DB_PATH, CONTEXT_DICTIONARIES
from .helpers import log_info, log_error, tokenize as _tok, tfidf_vec as _tfidf_vec

class SemanticAnalyzer:
    """Klasa do zaawansowanej analizy semantycznej tekstu"""
    def __init__(self):
        self.sentiment_keywords = {
            "pozytywny": ["dobry", "≈õwietny", "doskona≈Çy", "zadowolony", "wspania≈Çy", "super", "fajny", "mi≈Çy", "ciekawy", "lubiƒô", "podoba", "polecam"],
            "negatywny": ["z≈Çy", "s≈Çaby", "kiepski", "niedobry", "rozczarowany", "niezadowolony", "fatalny", "beznadziejny", "okropny", "niestety", "problem"],
            "neutralny": ["normalny", "zwyk≈Çy", "standard", "≈õredni", "przeciƒôtny", "typowy"]
        }
        
        # S≈Çownik dla dok≈Çadniejszej analizy emocji
        self.emotion_keywords = {
            "rado≈õƒá": ["≈õwietny", "super", "zachwycajƒÖcy", "cieszyƒá", "uwielbiƒá", "rado≈õƒá", "szczƒô≈õliwy", "entuzjazm", "zadowolony", "radosny", "wow", "hurra"],
            "smutek": ["smutny", "przykro", "≈ºal", "szkoda", "p≈Çakaƒá", "przygnƒôbiony", "przykry", "smuci", "niestety", "rozczarowany", "porzuciƒá", "zrezygnowany"],
            "z≈Ço≈õƒá": ["wkurzony", "zdenerwowany", "w≈õciek≈Çy", "irytuje", "denerwuje", "z≈Çy", "zirytowany", "wkurza", "frustracja", "wkurzyƒá", "z≈Ço≈õciƒá"],
            "strach": ["boi siƒô", "przera≈ºony", "lƒôk", "obawy", "obawiam", "strach", "martwi", "zatrwo≈ºony", "niepewny", "przestraszony", "obawiam siƒô"],
            "zaskoczenie": ["wow", "zaskoczony", "zdziwiony", "niesamowity", "zaskakujƒÖcy", "niewiarygodny", "szok", "zdumiewajƒÖcy", "niezwyk≈Çy", "nieprawdopodobny"],
            "zaufanie": ["ufam", "wierzƒô", "polegam", "pewny", "sprawdzony", "bezpieczny", "wiarygodny", "niezawodny", "godny zaufania"],
            "wstrƒôt": ["obrzydliwy", "ohydny", "niesmaczny", "odra≈ºajƒÖcy", "paskudny", "wstrƒôtny", "niechƒôƒá", "okropny", "obrzydzenie"],
            "oczekiwanie": ["czekam", "oczekujƒô", "mam nadziejƒô", "spodziewaƒá siƒô", "przewidywaƒá", "liczyƒá", "powinno", "bƒôdzie", "chcia≈Çbym"]
        }
        
        self.intention_indicators = {
            "pytanie": ["?", "czy", "jak", "kiedy", "gdzie", "co", "dlaczego", "ile", "kt√≥ry", "jakie", "proszƒô wyja≈õniƒá"],
            "pro≈õba": ["proszƒô", "czy mo≈ºesz", "czy m√≥g≈Çby≈õ", "pom√≥≈º", "potrzebujƒô", "zr√≥b", "wykonaj", "daj", "poka≈º"],
            "stwierdzenie": ["jest", "sƒÖ", "my≈õlƒô", "sƒÖdzƒô", "uwa≈ºam", "moim zdaniem", "wydaje mi siƒô", "wiem", "rozumiem"]
        }
        
        # S≈Çowniki kategorii tematycznych
        self.topic_keywords = {
            "technologia": ["komputer", "laptop", "telefon", "internet", "aplikacja", "program", "software", "hardware", "kod", "programowanie"],
            "biznes": ["firma", "przedsiƒôbiorstwo", "zysk", "marketing", "sprzeda≈º", "klient", "produkt", "us≈Çuga", "rynek", "inwestycja"],
            "podr√≥≈ºe": ["wakacje", "wycieczka", "hotel", "rezerwacja", "lot", "samolot", "zwiedzanie", "turysta", "przewodnik", "destynacja"],
            "zdrowie": ["lekarz", "choroba", "lekarstwo", "terapia", "ƒáwiczenia", "dieta", "samopoczucie", "zdrowy", "pacjent", "dolegliwo≈õci"],
            "edukacja": ["szko≈Ça", "nauka", "studia", "uniwersytet", "kurs", "student", "profesor", "egzamin", "wyk≈Çad", "wiedza"],
            "rozrywka": ["film", "muzyka", "koncert", "spektakl", "ksiƒÖ≈ºka", "gra", "zabawa", "hobby", "serial", "festiwal"]
        }
        print("Analiza semantyczna - inicjalizacja powiod≈Ça siƒô")
        
    def analyze_text(self, text):
        """Kompleksowa analiza semantyczna tekstu"""
        if not text:
            return {}
            
        result = {
            "topics": self.detect_topics(text),
            "sentiment": self.analyze_sentiment(text),
            "emotions": self.analyze_emotions(text),
            "intention": self.detect_intention(text),
            "hidden_intentions": self.detect_hidden_intentions(text),
            "keywords": self.extract_keywords(text),
            "complexity": self.analyze_complexity(text),
            "temporal_context": self.detect_temporal_context(text),
            "entities": self.extract_entities(text)
        }
        return result
        
    def analyze_emotions(self, text):
        """Zaawansowana analiza emocji w tek≈õcie"""
        if not text:
            return {}
            
        text_lower = text.lower()
        tokens = _tok(text_lower) if hasattr(text_lower, '__len__') else []
        words = text_lower.split()
        
        # Analizuj emocje na podstawie s≈Ç√≥w kluczowych
        emotion_scores = {emotion: 0.0 for emotion in self.emotion_keywords}
        emotion_matches = {}
        
        # Zaimplementujmy podej≈õcie z uwzglƒôdnieniem kontekstu
        # Najpierw podstawowe zliczanie
        for emotion, keywords in self.emotion_keywords.items():
            matches = []
            for word in keywords:
                # Bardziej zaawansowane sprawdzenie ni≈º proste text_lower.count()
                if len(word.split()) > 1:  # Dla fraz wielowyrazowych
                    if word in text_lower:
                        matches.append(word)
                        emotion_scores[emotion] += 0.2
                else:  # Dla pojedynczych s≈Ç√≥w
                    # Dopasowanie form wyraz√≥w (np. smutek, smutny, smutno)
                    pattern = r"\b" + re.escape(word[:4]) + r"[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈∫]*\b"
                    matches_found = re.findall(pattern, text_lower)
                    if matches_found:
                        matches.extend(matches_found)
                        emotion_scores[emotion] += 0.1 * len(matches_found)
            
            if matches:
                emotion_matches[emotion] = matches
        
        # Analiza wzajemnych wzmocnie≈Ñ i os≈Çabie≈Ñ emocji
        if emotion_scores.get("rado≈õƒá", 0) > 0 and emotion_scores.get("smutek", 0) > 0:
            # Je≈õli wystƒôpuje jednocze≈õnie rado≈õƒá i smutek, sprawd≈∫my negacje
            if any(neg in text_lower for neg in ["nie jest", "nie by≈Ç", "nie sƒÖ", "nie czujƒô"]):
                # Prawdopodobnie negacja pozytywnych emocji
                if "nie" in text_lower and any(pos in text_lower[text_lower.find("nie"):] 
                                            for pos in self.emotion_keywords["rado≈õƒá"]):
                    emotion_scores["rado≈õƒá"] *= 0.3
                    emotion_scores["smutek"] *= 1.5
        
        # Uwzglƒôdnienie znak√≥w interpunkcyjnych i emotikon√≥w
        if "!" in text:
            # Wykrzykniki wzmacniajƒÖ dominujƒÖce emocje
            max_emotion = max(emotion_scores, key=emotion_scores.get)
            if max_emotion in ["rado≈õƒá", "z≈Ço≈õƒá", "zaskoczenie"]:
                emotion_scores[max_emotion] += 0.1 * text.count("!")
        
        # Emotikony i emoji
        happy_emojis = [":)", ":D", "üòä", "üòÅ", "üòÑ", "üëç"]
        sad_emojis = [":(", "üò¢", "üò≠", "üòî", "üëé"]
        angry_emojis = ["üò†", "üò°", "üëø", "üí¢"]
        surprised_emojis = ["üòÆ", "üòØ", "üò≤", "üò±", "üò≥"]
        
        for emoji in happy_emojis:
            count = text.count(emoji)
            if count > 0:
                emotion_scores["rado≈õƒá"] += 0.15 * count
                
        for emoji in sad_emojis:
            count = text.count(emoji)
            if count > 0:
                emotion_scores["smutek"] += 0.15 * count
                
        for emoji in angry_emojis:
            count = text.count(emoji)
            if count > 0:
                emotion_scores["z≈Ço≈õƒá"] += 0.15 * count
                
        for emoji in surprised_emojis:
            count = text.count(emoji)
            if count > 0:
                emotion_scores["zaskoczenie"] += 0.15 * count
        
        # Analiza intensywno≈õci na podstawie sk≈Çadni i powtarzajƒÖcych siƒô wzor√≥w
        intensity = 1.0
        if re.search(r"bardzo|niezwykle|ogromnie|niesamowicie|wyjƒÖtkowo", text_lower):
            intensity = 1.5
        elif re.search(r"trochƒô|lekko|nieco|delikatnie", text_lower):
            intensity = 0.7
            
        # Aplikujemy intensywno≈õƒá do wynik√≥w
        for emotion in emotion_scores:
            emotion_scores[emotion] *= intensity
        
        # Normalizacja wynik√≥w
        total = sum(emotion_scores.values()) or 1.0
        normalized = {k: round(v/total, 2) for k, v in emotion_scores.items() if v > 0}
        
        # DominujƒÖce emocje (top 3)
        dominant = sorted(normalized.items(), key=lambda x: x[1], reverse=True)[:3]
        dominant_emotions = {emotion: score for emotion, score in dominant if score > 0.1}
        
        return {
            "dominujƒÖce": dominant_emotions,
            "wszystkie": normalized,
            "intensywno≈õƒá": round(intensity, 2),
            "dopasowania": emotion_matches
        }
        
    def detect_topics(self, text):
        """Wykrywa tematy w tek≈õcie z wagami u≈ºywajƒÖc TF-IDF"""
        if not text:
            return {}
            
        text_lower = text.lower()
        text_tokens = _tok(text_lower)  # U≈ºywamy istniejƒÖcej funkcji tokenizujƒÖcej
        
        # Przygotowanie korpusu dokument√≥w do TF-IDF
        corpus = []
        topic_docs = {}
        
        # Tworzenie dokument√≥w dla ka≈ºdego tematu (dla TF-IDF)
        for topic, keywords in self.topic_keywords.items():
            topic_docs[topic] = " ".join(keywords)
            corpus.append(topic_docs[topic])
        
        # Dodaj zapytanie u≈ºytkownika jako ostatni dokument w korpusie
        corpus.append(text_lower)
        
        # Obliczenie wektor√≥w TF-IDF
        tfidf_scores = _tfidf_vec(text_tokens, [_tok(doc) for doc in corpus])
        
        # Obliczenie podobie≈Ñstwa miƒôdzy tekstem a tematami
        topic_scores = {}
        for topic, topic_text in topic_docs.items():
            topic_tokens = _tok(topic_text)
            topic_tfidf = _tfidf_vec(topic_tokens, [_tok(doc) for doc in corpus])
            
            # Iloczyn skalarny wektor√≥w TF-IDF (prostszy odpowiednik cosine similarity)
            score = 0
            for term in set(text_tokens) & set(topic_tokens):  # Wsp√≥lne terminy
                score += tfidf_scores.get(term, 0) * topic_tfidf.get(term, 0) * 3.0  # Waga dla wsp√≥lnych termin√≥w
                
            # Dodatkowa korekta dla s≈Ç√≥w kluczowych
            for keyword in self.topic_keywords[topic]:
                if keyword in text_lower:
                    score += 0.15  # Bonus za dok≈Çadne dopasowanie s≈Ç√≥w kluczowych
            
            if score > 0.1:  # Minimalny pr√≥g
                topic_scores[topic] = min(0.95, score)  # Ograniczenie maksymalnej warto≈õci
        
        # Dodatkowa analiza kontekstualna
        # Wzorce zakupowe
        if re.search(r'\b(kup|kupi[ƒáƒôc≈Ç]|zam[√≥o]wi[ƒáƒôc≈Ç]|sprzeda[ƒáƒôc≈Ç]|cen[ayƒô]|koszt|ofert[ayƒô]|tani|drogi)\b', text_lower):
            topic_scores["zakupy"] = max(topic_scores.get("zakupy", 0), 0.7)
            
        # Wzorce wsparcia technicznego
        if re.search(r'\b(problem|trudno[≈õsƒá][ƒácƒô]|b[≈Çl][ƒÖa]d|nie dzia[≈Çl]a|zepsut|pom[√≥o][≈ºz])\b', text_lower):
            topic_scores["wsparcie"] = max(topic_scores.get("wsparcie", 0), 0.75)
            
        # Wzorce finansowe
        if re.search(r'\b(pieni[ƒÖa]dz|z[≈Çl]ot|pln|eur|usd|walut|bank|konto|p[≈Çl]atno[≈õsƒá][ƒác])\b', text_lower):
            topic_scores["finanse"] = max(topic_scores.get("finanse", 0), 0.7)
            
        # Normalizacja wynik√≥w
        total_score = sum(topic_scores.values()) or 1.0
        for topic in topic_scores:
            topic_scores[topic] = topic_scores[topic] / total_score * 0.8 + 0.1  # Skalowanie do sensownego zakresu
            
        # Usu≈Ñ tematy z bardzo niskim wynikiem
        return {k: round(v, 2) for k, v in topic_scores.items() if v > 0.22}
    
    def analyze_sentiment(self, text):
        """Analiza sentymentu tekstu"""
        text_lower = text.lower()
        scores = {"pozytywny": 0, "negatywny": 0, "neutralny": 0}
        
        # Liczenie wystƒÖpie≈Ñ s≈Ç√≥w z ka≈ºdej kategorii
        for sentiment, words in self.sentiment_keywords.items():
            for word in words:
                count = text_lower.count(word)
                if count > 0:
                    scores[sentiment] += count * 0.1  # Ka≈ºde wystƒÖpienie zwiƒôksza wynik
        
        # Analiza znak√≥w interpunkcyjnych i emoji
        if "!" in text:
            excl_count = text.count("!")
            if scores["pozytywny"] > scores["negatywny"]:
                scores["pozytywny"] += excl_count * 0.05
            elif scores["negatywny"] > scores["pozytywny"]:
                scores["negatywny"] += excl_count * 0.05
                
        # Sprawd≈∫ emoji lub emotikony
        positive_emotes = [":)", ":D", "üòä", "üëç", "üòÅ"]
        negative_emotes = [":(", ":(", "üò¢", "üëé", "üò†"]
        
        for emote in positive_emotes:
            scores["pozytywny"] += text.count(emote) * 0.15
            
        for emote in negative_emotes:
            scores["negatywny"] += text.count(emote) * 0.15
        
        # Sprawd≈∫ negacjƒô, kt√≥ra mo≈ºe odwracaƒá sentyment
        negation_words = ["nie", "bez", "nigdy", "≈ºaden"]
        for word in negation_words:
            pattern = word + " [\\w]+ "
            matches = re.findall(pattern, text_lower)
            if matches:
                # Zmniejsz wp≈Çyw pozytywnych s≈Ç√≥w po negacji
                scores["pozytywny"] *= 0.8
                scores["negatywny"] *= 1.2
                
        # Normalizacja wynik√≥w
        total = sum(scores.values()) or 1
        normalized = {k: round(v/total, 2) for k, v in scores.items()}
        
        # Okre≈õlenie dominujƒÖcego sentymentu
        dominant = max(normalized, key=normalized.get)
        normalized["dominujƒÖcy"] = dominant
        
        return normalized
        
    def detect_intention(self, text):
        """Wykrywanie intencji u≈ºytkownika"""
        text_lower = text.lower()
        scores = {"pytanie": 0, "pro≈õba": 0, "stwierdzenie": 0}
        
        # Sprawd≈∫ znaki zapytania
        if "?" in text:
            scores["pytanie"] += 0.6
        
        # Sprawdzanie wska≈∫nik√≥w intencji
        for intention, indicators in self.intention_indicators.items():
            for indicator in indicators:
                if indicator in text_lower:
                    scores[intention] += 0.15
        
        # Analiza struktury gramatycznej (podstawowa)
        if text_lower.startswith("czy") or text_lower.startswith("jak") or text_lower.startswith("kiedy"):
            scores["pytanie"] += 0.3
            
        if "proszƒô" in text_lower or "czy mo≈ºesz" in text_lower or text_lower.startswith("pom√≥≈º"):
            scores["pro≈õba"] += 0.3
            
        if "." in text and "?" not in text:
            scores["stwierdzenie"] += 0.2
            
        # Normalizacja wynik√≥w
        total = sum(scores.values()) or 1
        normalized = {k: round(v/total, 2) for k, v in scores.items()}
        
        # Okre≈õlenie dominujƒÖcej intencji
        dominant = max(normalized, key=normalized.get)
        normalized["dominujƒÖca"] = dominant
        
        return normalized
    
    def extract_keywords(self, text):
        """Ekstrakcja s≈Ç√≥w kluczowych z tekstu"""
        # Proste czyszczenie tekstu
        text_lower = re.sub(r'[^\w\s]', ' ', text.lower())
        words = text_lower.split()
        
        # Lista stop words (podstawowa)
        stop_words = ["i", "w", "na", "z", "do", "od", "dla", "≈ºe", "to", "jest", "sƒÖ", "byƒá", "a", "o", "jak", "tak", "nie", "siƒô"]
        
        # Filtrowanie s≈Ç√≥w
        filtered_words = [word for word in words if word not in stop_words and len(word) > 2]
        
        # Obliczanie czƒôsto≈õci wystƒôpowania
        word_freq = {}
        for word in filtered_words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # Sortowanie po czƒôsto≈õci i zwracanie top N s≈Ç√≥w
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        top_keywords = [word for word, freq in sorted_words[:10]]
        
        return top_keywords
    
    def analyze_complexity(self, text):
        """Analiza z≈Ço≈ºono≈õci tekstu"""
        if not text:
            return {"poziom": "brak tekstu", "≈õrednia_d≈Çugo≈õƒá_zdania": 0, "≈õrednia_d≈Çugo≈õƒá_s≈Çowa": 0, "r√≥≈ºnorodno≈õƒá_leksykalna": 0}
            
        # Podzia≈Ç na zdania
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return {"poziom": "brak tekstu", "≈õrednia_d≈Çugo≈õƒá_zdania": 0, "≈õrednia_d≈Çugo≈õƒá_s≈Çowa": 0, "r√≥≈ºnorodno≈õƒá_leksykalna": 0}
        
        # Liczba s≈Ç√≥w w zdaniach
        words_per_sentence = [len(s.split()) for s in sentences]
        avg_sentence_length = sum(words_per_sentence) / len(sentences) if sentences else 0
        
        # ≈örednia d≈Çugo≈õƒá s≈Çowa
        all_words = [word for s in sentences for word in s.split()]
        if not all_words:
            return {"poziom": "brak tekstu", "≈õrednia_d≈Çugo≈õƒá_zdania": 0, "≈õrednia_d≈Çugo≈õƒá_s≈Çowa": 0, "r√≥≈ºnorodno≈õƒá_leksykalna": 0}
            
        avg_word_length = sum(len(word) for word in all_words) / len(all_words)
        
        # R√≥≈ºnorodno≈õƒá leksykalna (unique words / total words)
        lexical_diversity = len(set(all_words)) / len(all_words) if all_words else 0
        
        # Okre≈õlenie poziomu z≈Ço≈ºono≈õci
        complexity_level = "niska"
        if avg_sentence_length > 15 or avg_word_length > 6 or lexical_diversity > 0.7:
            complexity_level = "wysoka"
        elif avg_sentence_length > 10 or avg_word_length > 5 or lexical_diversity > 0.5:
            complexity_level = "≈õrednia"
            
        return {
            "poziom": complexity_level,
            "≈õrednia_d≈Çugo≈õƒá_zdania": round(avg_sentence_length, 2),
            "≈õrednia_d≈Çugo≈õƒá_s≈Çowa": round(avg_word_length, 2),
            "r√≥≈ºnorodno≈õƒá_leksykalna": round(lexical_diversity, 2)
        }
        
    def analyze_local_context(self, text):
        """Analizuje lokalny kontekst w tek≈õcie - lokalizacje, czas, odniesienia"""
        if not text:
            return {"lokalizacje": [], "czas": [], "odniesienia_przestrzenne": [], 
                    "odniesienia_czasowe": []}
            
        text_lower = text.lower()
        
        # S≈Çowniki do rozpoznawania rodzaj√≥w kontekstu
        # Lokalizacje (miasta, kraje, regiony)
        location_patterns = [
            # "w Warszawie", "do Polski"
            r"\b(?:w|do|z)\s+([A-Z][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]{2,})\b",
            # Nazwy w≈Çasne (miasta, kraje)  
            r"\b([A-Z][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]{2,})\b",
            # Nazwy ulic
            r"\b(?:ulica|ulicy|ul\.|aleja|alei|al\.)\s+([A-Z][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+)\b"
        ]
        
        # Wyra≈ºenia czasowe
        time_patterns = [
            r"\b(\d{1,2}:\d{2})\b",  # Format godziny 12:30
            r"\b(\d{1,2})[:\.-]\s?(\d{2})\b",  # Format godziny z separatorem
            r"\bo\s+(?:godz(?:inie)?)\s+(\d{1,2})\b",  # "o godzinie 5"
            r"\b(?:rano|po\s+po≈Çudniu|wieczorem|w\s+nocy)\b"  # Pory dnia
        ]
        
        # Odniesienia przestrzenne
        spatial_references = [
            r"\b(?:na\s+prawo|na\s+lewo|nad|pod|obok|przy|przed|za|naprzeciw)\b",
            r"\b(?:w\s+pobli≈ºu|niedaleko|blisko)\b",
            r"\b(?:na\s+p√≥≈Çnoc|na\s+po≈Çudnie|na\s+wsch√≥d|na\s+zach√≥d)\b",
            r"\b(?:w\s+centrum|na\s+obrze≈ºach|na\s+peryferiach|w\s+≈õrodku)\b"
        ]
        
        # Odniesienia czasowe
        temporal_references = [
            r"\b(?:wczoraj|dzisiaj|jutro|pojutrze|za\s+tydzie≈Ñ)\b",
            r"\b(?:w\s+przysz≈Çym\s+tygodniu|w\s+przysz≈Çym\s+miesiƒÖcu)\b",
            r"\b(?:rano|po\s+po≈Çudniu|wieczorem|w\s+nocy|o\s+≈õwicie|o\s+zmierzchu)\b",
            r"\b(?:w\s+poniedzia≈Çek|we\s+wtorek|w\s+≈õrodƒô|w\s+czwartek)\b",
            r"\b(?:w\s+piƒÖtek|w\s+sobotƒô|w\s+niedzielƒô)\b",
            r"\b(\d{1,2})\s+(?:stycznia|lutego|marca|kwietnia|maja|czerwca)\b",
            r"\b(\d{1,2})\s+(?:lipca|sierpnia|wrze≈õnia|pa≈∫dziernika|listopada|grudnia)\b"
        ]
        
        # Rozpoznawanie lokalizacji
        locations = []
        for pattern in location_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                locations.extend([m[0] if isinstance(m, tuple) else m for m in matches])
        
        # Rozpoznawanie wyra≈ºe≈Ñ czasowych
        times = []
        for pattern in time_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                times.extend([m[0] if isinstance(m, tuple) else m for m in matches])
        
        # Rozpoznawanie odniesie≈Ñ przestrzennych
        spatial_refs = []
        for pattern in spatial_references:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                spatial_refs.extend(matches)
        
        # Rozpoznawanie odniesie≈Ñ czasowych
        temporal_refs = []
        for pattern in temporal_references:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                if isinstance(matches[0], tuple):
                    # Dla dat w formacie "15 stycznia 2023"
                    temporal_refs.extend([' '.join(filter(None, m)) for m in matches])
                else:
                    temporal_refs.extend(matches)
        
        # Dodatkowe przetwarzanie dla lokalizacji z przyimkami
        processed_locations = []
        for loc in locations:
            # Czy≈õcimy z przyimk√≥w i dodatkowych znak√≥w
            cleaned_loc = re.sub(r'^(?:w|do|z|na|przy)\s+', '', loc)
            cleaned_loc = re.sub(r'[,.:;"\'()]', '', cleaned_loc)
            if len(cleaned_loc) > 2:  # Minimalna d≈Çugo≈õƒá nazwy lokalizacji
                processed_locations.append(cleaned_loc)
        
        # Deduplikacja wynik√≥w
        locations = list(set(processed_locations))
        times = list(set(times))
        spatial_refs = list(set(spatial_refs))
        temporal_refs = list(set(temporal_refs))
        
        # Sortowanie wynik√≥w wed≈Çug d≈Çugo≈õci (d≈Çu≈ºsze nazwy sƒÖ czƒôsto bardziej specyficzne)
        locations.sort(key=len, reverse=True)
        
        # Usuwanie fa≈Çszywych trafie≈Ñ (typowe s≈Çowa, kt√≥re nie sƒÖ lokalizacjami)
        common_words = ["jako", "tego", "tych", "inne", "moje", "twoje", "nasze"]
        locations = [loc for loc in locations if loc.lower() not in common_words]
        
        # Identyfikacja g≈Ç√≥wnego kontekstu przestrzenno-czasowego
        main_location = locations[0] if locations else None
        main_time = temporal_refs[0] if temporal_refs else None
        
        return {
            "lokalizacje": locations,
            "czas": times,
            "odniesienia_przestrzenne": spatial_refs,
            "odniesienia_czasowe": temporal_refs,
            "g≈Ç√≥wna_lokalizacja": main_location,
            "g≈Ç√≥wny_czas": main_time
        }
    def analyze_discourse(self, text):
        """Analizuje dyskurs - identyfikuje typ, strukturƒô i cechy komunikacji"""
        if not text:
            return {"typ_dyskursu": "brak tekstu", "cechy": [], "s≈Çowa_kluczowe": []}
            
        text_lower = text.lower()
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        if not sentences:
            return {"typ_dyskursu": "brak tekstu", "cechy": [], "s≈Çowa_kluczowe": []}
            
        # S≈Çowniki do identyfikacji typ√≥w dyskursu
        discourse_markers = {
            "naukowy": [
                r"\b(?:badania|badanie|analiza|analizy|hipoteza|teoria|wyniki)\b",
                r"\b(?:dow√≥d|dowody|metodologia|eksperyment|dane|wniosek)\b",
                r"\b(?:wed≈Çug\s+(?:\w+\s+){0,2}et\s+al\.|cytujƒÖc|zgodnie\s+z)\b"
            ],
            "polityczny": [
                r"\b(?:pa≈Ñstwo|w≈Çadza|rzƒÖd|ustawa|prawo|spo≈Çecze≈Ñstwo)\b",
                r"\b(?:polityka|polityczny|partia|demokracja|wybory)\b",
                r"\b(?:obywatel|obywatele|obywatelski|konstytucja|wolno≈õƒá)\b"
            ],
            "biznesowy": [
                r"\b(?:firma|biznes|przedsiƒôbiorstwo|klient|klienci|zysk)\b",
                r"\b(?:sprzeda≈º|rynek|marketing|strategia|bud≈ºet|przych√≥d)\b",
                r"\b(?:produkt|us≈Çuga|warto≈õƒá|cena|oferta|umowa|kontrakt)\b"
            ],
            "potoczny": [
                r"\b(?:super|fajnie|ekstra|spoko|ziom|hej|cze≈õƒá|siema|nara)\b",
                r"\b(?:mega|totalnie|generalnie|jakby|wiesz|no\s+wiesz)\b",
                r"(?:!{2,}|\\?{2,})"
            ],
            "perswazyjny": [
                r"\b(?:musisz|powiniene≈õ|nale≈ºy|trzeba|koniecznie)\b",
                r"\b(?:najlepszy|jedyny|wyjƒÖtkowy|niesamowity|rewolucyjny)\b",
                r"\b(?:przekonaj\s+siƒô|sprawd≈∫|nie\s+przegap|ju≈º\s+dzi≈õ)\b"
            ],
            "emocjonalny": [
                r"\b(?:kocham|nienawidzƒô|uwielbiam|bojƒô\s+siƒô|tƒôskniƒô)\b",
                r"\b(?:rado≈õƒá|smutek|z≈Ço≈õƒá|strach|niepok√≥j|wzruszenie)\b",
                r"(?:!{2,}|\\?!|\\.{3,})"
            ],
            "informacyjny": [
                r"\b(?:informacja|informujƒô|zawiadamiam|komunikat|og≈Çoszenie)\b",
                r"\b(?:przekazujƒô|uprzejmie\s+informujƒô|podajƒô\s+do\s+wiadomo≈õci)\b",
                r"\b(?:dane|fakty|statystyki|zestawienie|podsumowanie)\b"
            ]
        }
        
        # Cechy dyskursu
        discourse_features = {
            "formalny": [
                r"\b(?:szanowny|uprzejmie|z\s+powa≈ºaniem|niniejszym)\b",
                r"\b(?:pragnƒô\s+podkre≈õliƒá|nale≈ºy\s+zaznaczyƒá)\b"
            ],
            "nieformalny": [
                r"\b(?:hej|cze≈õƒá|siema|s≈Çuchaj|wiesz\s+co|no\s+dobra|ok)\b",
                r"(?:!{2,}|\\?{2,})"
            ],
            "argumentacyjny": [
                r"\b(?:poniewa≈º|dlatego|zatem|wiƒôc|skutkiem)\b",
                r"\b(?:po\s+pierwsze|po\s+drugie|z\s+jednej\s+strony)\b",
                r"\b(?:argumentujƒô|twierdzƒô|uwa≈ºam|wnioskujƒô)\b"
            ],
            "narracyjny": [
                r"\b(?:pewnego\s+dnia|dawno\s+temu|na\s+poczƒÖtku)\b",
                r"\b(?:nastƒôpnie|po\s+chwili|tymczasem|w\s+ko≈Ñcu)\b"
            ],
            "dialogowy": [
                r"\b(?:pytam|odpowiadam|m√≥wiƒô|twierdzisz|sugerujesz)\b",
                r'''["‚Äû"''].*?["‚Äû"']''',
                r"\b(?:rozmowa|dialog|dyskusja|debata)\b"
            ],
            "opisowy": [
                r"\b(?:jest|by≈Ç|znajdowa≈Ç\s+siƒô|wyglƒÖda≈Ç|przypomina≈Ç)\b",
                r"\b(?:wysoki|szeroki|ciemny|jasny|czerwony|du≈ºy)\b"
            ],
            "instrukta≈ºowy": [
                r"\b(?:najpierw|nastƒôpnie|potem|na\s+koniec|krok)\b",
                r"\b(?:w≈ÇƒÖcz|wy≈ÇƒÖcz|naci≈õnij|kliknij|otw√≥rz|zamknij)\b",
                r"(?:^\s*\d+\.|^\s*-|\*\s)"
            ]
        }
        
        # Analiza typu dyskursu
        discourse_scores = {}
        for disc_type, patterns in discourse_markers.items():
            score = 0
            for pattern in patterns:
                matches = re.findall(pattern, text_lower, re.MULTILINE)
                score += len(matches)
            discourse_scores[disc_type] = score
            
        # Analiza cech dyskursu
        features = []
        for feature, patterns in discourse_features.items():
            score = 0
            for pattern in patterns:
                matches = re.findall(pattern, text_lower, re.MULTILINE)
                score += len(matches)
            if score >= 2:  # Pr√≥g minimalny dla uznania cechy
                features.append(feature)
                
        # Struktura dyskursu - analiza po≈ÇƒÖcze≈Ñ logicznych
        logical_connectors = [
            r"\b(?:poniewa≈º|bo|gdy≈º|dlatego|wiƒôc|zatem|stƒÖd)\b",
            r"\b(?:je≈õli|je≈ºeli|o\s+ile|pod\s+warunkiem)\b",
            r"\b(?:ale|lecz|jednak|niemniej|natomiast)\b",
            r"\b(?:po\s+pierwsze|po\s+drugie|przede\s+wszystkim)\b"
        ]
        
        connectors_count = 0
        for pattern in logical_connectors:
            connectors_count += len(re.findall(pattern, text_lower))
            
        # Gƒôsto≈õƒá logiczna - liczba po≈ÇƒÖcze≈Ñ logicznych na zdanie
        logical_density = connectors_count / len(sentences) if sentences else 0
        
        # Kompleksowo≈õƒá dyskursu - ≈õrednia d≈Çugo≈õƒá zdania
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0
        complexity_words = re.findall(r'\b\w{10,}\b', text_lower)
        lexical_complexity = len(complexity_words) / len(sentences) if sentences else 0
        
        # Okre≈õlenie g≈Ç√≥wnego typu dyskursu
        main_discourse_type = max(discourse_scores.items(), key=lambda x: x[1])[0] \
            if any(score > 0 for score in discourse_scores.values()) else "nieokre≈õlony"
            
        # S≈Çowa kluczowe w dyskursie
        words = re.findall(r'\b\w+\b', text_lower)
        word_freq = {}
        for word in words:
            if len(word) > 3:  # Pomijamy kr√≥tkie s≈Çowa
                word_freq[word] = word_freq.get(word, 0) + 1
                
        # Lista polskich stopwords (s≈Ç√≥w nieistotnych)
        stopwords = [
            "oraz", "jako", "tylko", "tego", "przez", "jest", "jestem", 
            "jeste≈õmy", "poniewa≈º", "≈ºeby", "kt√≥ry", "kt√≥ra", "kt√≥re", 
            "tak≈ºe", "r√≥wnie≈º", "dlatego", "wiƒôc", "czyli", "gdy≈º", "albo",
            "czyli", "lecz", "gdy≈º", "oraz", "jednak", "choƒá"
        ]
        
        # Filtrowanie s≈Ç√≥w nieistotnych
        for word in stopwords:
            if word in word_freq:
                del word_freq[word]
                
        # Wybieranie najczƒôstszych s≈Ç√≥w jako s≈Çowa kluczowe
        keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        keywords = [word for word, freq in keywords]
        
        result = {
            "typ_dyskursu": main_discourse_type,
            "cechy": features,
            "s≈Çowa_kluczowe": keywords,
            "gƒôsto≈õƒá_logiczna": round(logical_density, 2),
            "z≈Ço≈ºono≈õƒá_leksykalna": round(lexical_complexity, 2),
            "≈õrednia_d≈Çugo≈õƒá_zdania": round(avg_sentence_length, 2)
        }
        
        # Dodanie oceny jako≈õci dyskursu
        if logical_density > 0.5 and lexical_complexity > 0.3 and avg_sentence_length > 15:
            result["ocena_jako≈õci"] = "zaawansowany"
        elif logical_density > 0.3 and avg_sentence_length > 10:
            result["ocena_jako≈õci"] = "standardowy"
        else:
            result["ocena_jako≈õci"] = "prosty"
            
        return result
        
    def analyze_arguments(self, text):
        """Analizuje strukturƒô argumentacyjnƒÖ tekstu"""
        if not text:
            return {"struktura": "brak tekstu", "elementy": [], "jako≈õƒá": "brak"}
            
        # Dzielimy tekst na zdania
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        if not sentences:
            return {"struktura": "brak tekstu", "elementy": [], "jako≈õƒá": "brak"}
        
        # Wzorce dla rozpoznawania element√≥w argumentacji
        argument_patterns = {
            "teza_g≈Ç√≥wna": [
                r"\b(?:uwa≈ºam,\s+≈ºe|twierdzƒô,\s+≈ºe|moim\s+zdaniem)\b",
                r"\b(?:chcia≈Ç[a]?bym\s+dowie≈õƒá|zamierzam\s+pokazaƒá)\b",
                r"\b(?:g≈Ç√≥wn[ym|ƒÖ]\s+(?:tez[ƒÖ|e]|kwesti[ƒÖ|e])\s+jest)\b"
            ],
            "przes≈Çanka": [
                r"\b(?:poniewa≈º|gdy≈º|bowiem|dlatego\s+≈ºe|z\s+powodu)\b",
                r"\b(?:pierwszym\s+argumentem|drugim\s+argumentem)\b",
                r"\b(?:dowodzi\s+tego|≈õwiadczy\s+o\s+tym|potwierdza\s+to)\b"
            ],
            "kontrargument": [
                r"\b(?:jednak|niemniej\s+jednak|z\s+drugiej\s+strony)\b",
                r"\b(?:mo≈ºna\s+(?:by|te≈º)\s+(?:zauwa≈ºyƒá|argumentowaƒá))\b",
                r"\b(?:przeciwnicy\s+twierdzƒÖ|krytycy\s+wskazujƒÖ)\b"
            ],
            "konkluzja": [
                r"\b(?:w\s+(?:konsekwencji|rezultacie|efekcie))\b",
                r"\b(?:(?:podsumowujƒÖc|reasumujƒÖc|konkludujƒÖc))\b",
                r"\b(?:(?:ostatecznie|finalnie|w\s+konkluzji))\b"
            ],
            "przyk≈Çad": [
                r"\b(?:na\s+przyk≈Çad|przyk≈Çadem\s+jest|dla\s+przyk≈Çadu)\b",
                r"\b(?:doskonale\s+ilustruje\s+to|≈õwiadczy\s+o\s+tym)\b",
                r"\b(?:warto\s+(?:przytoczyƒá|wskazaƒá)\s+przyk≈Çad)\b"
            ],
            "definicja": [
                r"\b(?:definiujƒô|rozumiem\s+(?:przez|jako)|oznacza\s+to)\b",
                r"\b(?:termin|pojƒôcie)\s+(?:\w+)\s+(?:odnosi\s+siƒô|oznacza)\b",
                r"(?:(?:^|[.!?]\s+)(?:[A-Z]\w+)\s+(?:to|jest|oznacza))\b"
            ]
        }
        
        # Sp√≥jniki logiczne i ich kategorie
        logical_connectors = {
            "przyczynowe": [
                r"\b(?:poniewa≈º|gdy≈º|bowiem|dlatego\s+≈ºe|z\s+powodu)\b",
                r"\b(?:w\s+zwiƒÖzku\s+z\s+tym|skutkiem\s+tego)\b"
            ],
            "kontrastujƒÖce": [
                r"\b(?:jednak|niemniej|natomiast|ale|lecz|choƒá|chocia≈º)\b",
                r"\b(?:z\s+drugiej\s+strony|przeciwnie|wbrew\s+temu)\b"
            ],
            "wynikowe": [
                r"\b(?:w\s+rezultacie|w\s+efekcie|w\s+konsekwencji)\b",
                r"\b(?:zatem|wiƒôc|tak\s+wiƒôc|stƒÖd|dlatego)\b"
            ],
            "wzmacniajƒÖce": [
                r"\b(?:co\s+wiƒôcej|ponadto|dodatkowo|w\s+dodatku)\b",
                r"\b(?:nie\s+tylko|r√≥wnie≈º|tak≈ºe|zar√≥wno)\b"
            ],
            "porzƒÖdkujƒÖce": [
                r"\b(?:po\s+pierwsze|po\s+drugie|nastƒôpnie|w\s+ko≈Ñcu)\b",
                r"\b(?:przede\s+wszystkim|w\s+szczeg√≥lno≈õci|g≈Ç√≥wnie)\b"
            ]
        }
        
        # Identyfikacja element√≥w argumentacji w zdaniach
        argument_structure = {}
        for arg_type, patterns in argument_patterns.items():
            argument_structure[arg_type] = []
            for pattern in patterns:
                for i, sentence in enumerate(sentences):
                    if re.search(pattern, sentence, re.IGNORECASE):
                        argument_structure[arg_type].append({
                            "zdanie": sentence,
                            "pozycja": i + 1
                        })
        
        # Identyfikacja sp√≥jnik√≥w logicznych
        connectors_found = {}
        for conn_type, patterns in logical_connectors.items():
            connectors_found[conn_type] = 0
            for pattern in patterns:
                for sentence in sentences:
                    matches = re.findall(pattern, sentence, re.IGNORECASE)
                    connectors_found[conn_type] += len(matches)
        
        # Okre≈õlanie struktury argumentacyjnej
        structure_type = "nieokre≈õlona"
        elements_found = []
        
        # Sprawdzanie kompletno≈õci argumentacji
        if argument_structure["teza_g≈Ç√≥wna"] and argument_structure["konkluzja"]:
            if argument_structure["przes≈Çanka"]:
                if argument_structure["kontrargument"]:
                    structure_type = "z≈Ço≈ºona dialektyczna"
                    elements_found = ["teza", "przes≈Çanki", "kontrargumenty", "konkluzja"]
                else:
                    structure_type = "prosta liniowa"
                    elements_found = ["teza", "przes≈Çanki", "konkluzja"]
            else:
                structure_type = "niekompletna"
                elements_found = ["teza", "konkluzja"]
        elif argument_structure["przes≈Çanka"]:
            if argument_structure["teza_g≈Ç√≥wna"]:
                structure_type = "niedoko≈Ñczona"
                elements_found = ["teza", "przes≈Çanki"]
            elif argument_structure["konkluzja"]:
                structure_type = "indukcyjna"
                elements_found = ["przes≈Çanki", "konkluzja"]
            else:
                structure_type = "fragmentaryczna"
                elements_found = ["przes≈Çanki"]
        elif argument_structure["teza_g≈Ç√≥wna"]:
            structure_type = "deklaratywna"
            elements_found = ["teza"]
        
        # Okre≈õlanie jako≈õci argumentacji
        arg_quality = "niska"
        
        # Liczenie element√≥w argumentacji
        total_elements = sum(len(items) for items in argument_structure.values())
        
        # Sprawdzanie obecno≈õci definicji i przyk≈Çad√≥w
        has_definitions = len(argument_structure["definicja"]) > 0
        has_examples = len(argument_structure["przyk≈Çad"]) > 0
        
        # Liczenie sp√≥jnik√≥w logicznych
        total_connectors = sum(connectors_found.values())
        
        # Ocena jako≈õci argumentacji
        conn_per_sentence = total_connectors / len(sentences) if sentences else 0
        
        # Zr√≥≈ºnicowanie typ√≥w sp√≥jnik√≥w
        connector_diversity = sum(1 for count in connectors_found.values() if count > 0)
        
        # Kryteria jako≈õci
        if (structure_type in ["z≈Ço≈ºona dialektyczna", "prosta liniowa"] and 
                has_definitions and has_examples and conn_per_sentence >= 0.5 and
                connector_diversity >= 3):
            arg_quality = "wysoka"
        elif (total_elements >= 5 and conn_per_sentence >= 0.3 and
              (has_definitions or has_examples) and connector_diversity >= 2):
            arg_quality = "≈õrednia"
        
        # Identyfikacja g≈Ç√≥wnych argument√≥w
        main_args = []
        for arg_type in ["teza_g≈Ç√≥wna", "przes≈Çanka", "konkluzja"]:
            for item in argument_structure[arg_type]:
                if item not in main_args:
                    main_args.append(item["zdanie"])
        
        result = {
            "struktura": structure_type,
            "elementy": elements_found,
            "g≈Ç√≥wne_argumenty": main_args[:3],  # Ograniczamy do 3 najwa≈ºniejszych
            "jako≈õƒá": arg_quality,
            "sp√≥jniki_logiczne": {
                "liczba": total_connectors,
                "na_zdanie": round(conn_per_sentence, 2),
                "rodzaje": {k: v for k, v in connectors_found.items() if v > 0}
            }
        }
        
        # Dodajemy ocenƒô balansu argumentacji
        if argument_structure["kontrargument"]:
            contra_to_pro_ratio = (len(argument_structure["kontrargument"]) / 
                                 len(argument_structure["przes≈Çanka"]) 
                                 if argument_structure["przes≈Çanka"] else 0)
            result["balans_argumentacji"] = round(contra_to_pro_ratio, 2)
            
            if 0.3 <= contra_to_pro_ratio <= 0.7:
                result["ocena_balansu"] = "zr√≥wnowa≈ºona"
            elif contra_to_pro_ratio > 0.7:
                result["ocena_balansu"] = "silnie dialektyczna"
            else:
                result["ocena_balansu"] = "jednostronna"
        else:
            result["balans_argumentacji"] = 0.0
            result["ocena_balansu"] = "jednokierunkowa"
            
        return result
        
    def analyze_semantic_structure(self, text):
        """Analizuje g≈ÇƒôbokƒÖ strukturƒô semantycznƒÖ tekstu"""
        if not text:
            return {"struktura": "brak tekstu", "relacje": [], "tematy": []}
            
        text_lower = text.lower()
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        if not sentences:
            return {"struktura": "brak tekstu", "relacje": [], "tematy": []}
            
        # 1. Analiza podmiot√≥w i obiekt√≥w w tek≈õcie
        entities = []
        patterns = {
            "osoba": [
                r"\b([A-Z][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+)\b(?=\s+(?:powiedzia≈Ç|stwierdzi≈Ç|uwa≈ºa))",
                r"\b([A-Z][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+)\b(?=\s+(?:jest|by≈Ç|bƒôdzie))"
            ],
            "organizacja": [
                r"\b([A-Z][A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ªa-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+)\b(?=\s+(?:og≈Çosi≈Ç|poinformowa≈Ç))",
                r"\b(?:firma|sp√≥≈Çka|organizacja|instytucja|ministerstwo)\s+([A-Z]\w+)\b"
            ],
            "miejsce": [
                r"\bw\s+([A-Z][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+)\b",
                r"\bdo\s+([A-Z][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+)\b",
                r"\bz\s+([A-Z][a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+)\b"
            ],
            "czas": [
                r"\b(\d{1,2}\s+(?:stycznia|lutego|marca|kwietnia|maja|czerwca|lipca|sierpnia|wrze≈õnia|pa≈∫dziernika|listopada|grudnia)(?:\s+\d{4})?)\b",
                r"\b((?:w\s+)?(?:poniedzia≈Çek|wtorek|≈õrodƒô|czwartek|piƒÖtek|sobotƒô|niedzielƒô))\b",
                r"\b(\d{1,2}:\d{2})\b"
            ],
            "pojƒôcie": [
                r"\b([a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]{5,}(?:acja|izm|o≈õƒá|stwo|ctwo|anie|enie))\b"
            ]
        }
        
        for entity_type, patterns_list in patterns.items():
            for pattern in patterns_list:
                for sentence in sentences:
                    matches = re.findall(pattern, sentence, re.IGNORECASE)
                    for match in matches:
                        if isinstance(match, tuple):
                            match = match[0]
                        if len(match) > 2:  # Minimalna d≈Çugo≈õƒá encji
                            entities.append({
                                "tekst": match,
                                "typ": entity_type,
                                "kontekst": sentence[:50] + "..." if len(sentence) > 50 else sentence
                            })
        
        # Filtrowanie duplikat√≥w
        unique_entities = []
        seen_entities = set()
        for entity in entities:
            key = (entity["tekst"].lower(), entity["typ"])
            if key not in seen_entities:
                seen_entities.add(key)
                unique_entities.append(entity)
        
        # 2. Analiza relacji semantycznych
        relations = []
        semantic_patterns = {
            "przyczynowo-skutkowe": [
                r"(\b\w+[^.!?]*)\s+(?:powoduje|powodujƒÖ|spowodowa≈Ç|spowodowa≈Ça|wywo≈Çuje|skutkuje)\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:wp≈Çywa|wp≈ÇywajƒÖ|wp≈ÇynƒÖ≈Ç|wp≈Çynƒô≈Ça)\s+na\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:dlatego|z\s+tego\s+powodu|w\s+rezultacie)\s+([^.!?]*)"
            ],
            "por√≥wnawcze": [
                r"(\b\w+[^.!?]*)\s+(?:podobnie\s+jak|tak\s+jak|podobnie\s+do)\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:r√≥≈ºni\s+siƒô\s+od|jest\s+inne\s+ni≈º|jest\s+odmienne\s+od)\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:w\s+przeciwie≈Ñstwie\s+do)\s+([^.!?]*)"
            ],
            "czƒô≈õƒá-ca≈Ço≈õƒá": [
                r"(\b\w+[^.!?]*)\s+(?:sk≈Çada\s+siƒô\s+z|zawiera|obejmuje)\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:jest\s+czƒô≈õciƒÖ|wchodzi\s+w\s+sk≈Çad|nale≈ºy\s+do)\s+([^.!?]*)"
            ],
            "posesywne": [
                r"(\b\w+[^.!?]*)\s+(?:posiada|ma|dysponuje)\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:nale≈ºƒÖcy\s+do|w≈Ça≈õciciel|w≈Çasno≈õƒá)\s+([^.!?]*)"
            ],
            "temporalne": [
                r"(\b\w+[^.!?]*)\s+(?:przed|po|w\s+trakcie|podczas)\s+([^.!?]*)",
                r"(\b\w+[^.!?]*)\s+(?:wcze≈õniej\s+ni≈º|p√≥≈∫niej\s+ni≈º|r√≥wnocze≈õnie\s+z)\s+([^.!?]*)"
            ]
        }
        
        for rel_type, patterns_list in semantic_patterns.items():
            for pattern in patterns_list:
                for sentence in sentences:
                    matches = re.findall(pattern, sentence, re.IGNORECASE)
                    for match in matches:
                        if len(match) >= 2:
                            relations.append({
                                "typ": rel_type,
                                "element_1": match[0].strip(),
                                "element_2": match[1].strip(),
                                "zdanie": sentence
                            })
        
        # 3. Analiza struktury tematycznej
        topic_words = {}
        
        # Ekstrakcja rzeczownik√≥w jako potencjalnych temat√≥w
        noun_pattern = r"\b([a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]{3,}(?:o≈õƒá|anie|enie|stwo|ctwo|cja|zja|acja|izm|or|er|yk))\b"
        for sentence in sentences:
            matches = re.findall(noun_pattern, sentence.lower())
            for match in matches:
                if match not in topic_words:
                    topic_words[match] = 0
                topic_words[match] += 1
        
        # Lista "stop words" dla temat√≥w
        stop_words = ["dlatego", "poniewa≈º", "przez", "gdy≈º", "czyli", "wiƒôc", "jednak", 
                     "bowiem", "tak≈ºe", "r√≥wnie≈º", "czyli", "w≈Ça≈õnie", "natomiast"]
        
        # Filtrowanie potencjalnych temat√≥w
        for word in stop_words:
            if word in topic_words:
                del topic_words[word]
        
        # Wyb√≥r g≈Ç√≥wnych temat√≥w
        main_topics = sorted(topic_words.items(), key=lambda x: x[1], reverse=True)[:5]
        main_topics = [{"temat": topic, "czƒôsto≈õƒá": count} for topic, count in main_topics]
        
        # 4. Analiza kohezji tekstu (powiƒÖza≈Ñ wewnƒôtrznych)
        cohesion_markers = {
            "zaimki_anaforyczne": [
                r"\b(?:on[a|i]?|jeg[a|o]|jej|ich|t[en|ƒÖ|ym|ymi]|t[a|e]|ci|tamci|tamte)\b"
            ],
            "odniesienia_tematyczne": [
                r"\b(?:ten\s+sam|wspomnian[y|a|e]|powy≈ºsz[y|a|e]|wcze≈õniejsz[y|a|e])\b"
            ],
            "sp√≥jniki_kontynuacji": [
                r"\b(?:ponadto|poza\s+tym|co\s+wiƒôcej|nastƒôpnie|dalej|kontynuujƒÖc)\b"
            ],
            "powt√≥rzenia_leksykalne": []  # Bƒôdzie analizowane algorytmicznie
        }
        
        # Liczenie marker√≥w kohezji
        cohesion_counts = {}
        for marker_type, patterns_list in cohesion_markers.items():
            cohesion_counts[marker_type] = 0
            for pattern in patterns_list:
                for sentence in sentences:
                    matches = re.findall(pattern, sentence, re.IGNORECASE)
                    cohesion_counts[marker_type] += len(matches)
        
        # Analiza powt√≥rze≈Ñ leksykalnych
        words_by_sentence = [re.findall(r'\b\w{3,}\b', s.lower()) for s in sentences]
        repetitions = 0
        
        # Szukamy powt√≥rze≈Ñ s≈Ç√≥w miƒôdzy zdaniami
        if len(words_by_sentence) > 1:
            for i in range(1, len(words_by_sentence)):
                for word in words_by_sentence[i]:
                    if word in words_by_sentence[i-1]:
                        repetitions += 1
        
        cohesion_counts["powt√≥rzenia_leksykalne"] = repetitions
        
        # Og√≥lna miara kohezji
        cohesion_total = sum(cohesion_counts.values())
        cohesion_per_sentence = cohesion_total / len(sentences) if sentences else 0
        
        # Okre≈õlenie sp√≥jno≈õci semantycznej
        cohesion_level = "niska"
        if cohesion_per_sentence >= 1.5:
            cohesion_level = "wysoka"
        elif cohesion_per_sentence >= 0.8:
            cohesion_level = "≈õrednia"
        
        # 5. Okre≈õlenie g≈Ç√≥wnej struktury semantycznej
        semantic_structure_types = {
            "narracyjna": 0,
            "ekspozycyjna": 0, 
            "argumentacyjna": 0,
            "opisowa": 0,
            "instrukta≈ºowa": 0
        }
        
        # Wzorce jƒôzykowe charakterystyczne dla poszczeg√≥lnych struktur
        structure_patterns = {
            "narracyjna": [
                r"\b(?:najpierw|potem|nastƒôpnie|wtedy|p√≥≈∫niej|w\s+ko≈Ñcu)\b",
                r"\b(?:gdy|kiedy|podczas|po\s+tym\s+jak|zanim|wkr√≥tce)\b",
                r"\b(?:pewnego\s+dnia|pewnego\s+razu|dawno\s+temu|kiedy≈õ)\b"
            ],
            "ekspozycyjna": [
                r"\b(?:definiuje|klasyfikuje|wyja≈õnia|przedstawia|omawia)\b",
                r"\b(?:po\s+pierwsze|po\s+drugie|jednym\s+z|kolejnym)\b",
                r"\b(?:g≈Ç√≥wnym|kluczowym|istotnym|wa≈ºnym|podstawowym)\b"
            ],
            "argumentacyjna": [
                r"\b(?:twierdzƒô|uwa≈ºam|sƒÖdzƒô|dowodzƒô|argumentujƒô|przekonujƒô)\b",
                r"\b(?:poniewa≈º|dlatego|zatem|wobec\s+tego|wynika\s+z\s+tego)\b",
                r"\b(?:podsumowujƒÖc|w\s+konkluzji|z\s+tego\s+wynika|dowodzi\s+to)\b"
            ],
            "opisowa": [
                r"\b(?:wyglƒÖda\s+jak|przypomina|charakteryzuje\s+siƒô|cechuje\s+siƒô)\b",
                r"\b(?:jest|wydaje\s+siƒô|sprawia\s+wra≈ºenie|prezentuje\s+siƒô\s+jako)\b",
                r"\b(?:czerwony|niebieski|zielony|du≈ºy|ma≈Çy|szeroki|wƒÖski|wysoki)\b"
            ],
            "instrukta≈ºowa": [
                r"\b(?:nale≈ºy|trzeba|powinno\s+siƒô|musisz|najpierw|nastƒôpnie)\b",
                r"\b(?:krok\s+po\s+kroku|w\s+pierwszej\s+kolejno≈õci|na\s+ko≈Ñcu)\b",
                r"(?:^\s*\d+\.|\d\)\s+|\-\s+|‚Ä¢\s+)"
            ]
        }
        
        # Analiza wzorc√≥w dla okre≈õlenia struktury
        for structure, patterns_list in structure_patterns.items():
            for pattern in patterns_list:
                for sentence in sentences:
                    matches = re.findall(pattern, sentence, re.IGNORECASE)
                    semantic_structure_types[structure] += len(matches)
        
        main_structure = max(semantic_structure_types.items(), key=lambda x: x[1])
        main_structure_type = main_structure[0]
        if main_structure[1] == 0:
            main_structure_type = "mieszana/nieokre≈õlona"
        
        result = {
            "struktura_semantyczna": main_structure_type,
            "encje": unique_entities[:10],  # Ograniczamy do top 10
            "relacje": relations[:10],      # Ograniczamy do top 10
            "g≈Ç√≥wne_tematy": main_topics,
            "sp√≥jno≈õƒá": {
                "poziom": cohesion_level,
                "markery_kohezji": cohesion_counts,
                "wska≈∫nik_sp√≥jno≈õci": round(cohesion_per_sentence, 2)
            }
        }
        
        return result
    
    def detect_temporal_context(self, text):
        """Wykrywanie kontekstu czasowego w tek≈õcie"""
        text_lower = text.lower()
        temporal_scores = {"przesz≈Ço≈õƒá": 0, "tera≈∫niejszo≈õƒá": 0.1, "przysz≈Ço≈õƒá": 0}
        
        # Wska≈∫niki czasu przesz≈Çego
        past_indicators = ["by≈Ç", "by≈Ça", "by≈Ço", "by≈Çy", "by≈Çem", "by≈Çam", "zrobi≈Çem", "zrobi≈Çam", "wczoraj", "wcze≈õniej", "dawniej", "kiedy≈õ", "niedawno"]
        
        # Wska≈∫niki czasu tera≈∫niejszego
        present_indicators = ["jest", "sƒÖ", "jestem", "jeste≈õ", "robimy", "robiƒô", "teraz", "obecnie", "dzi≈õ", "dzisiaj"]
        
        # Wska≈∫niki czasu przysz≈Çego
        future_indicators = ["bƒôdzie", "bƒôdƒÖ", "bƒôdƒô", "bƒôdziemy", "zrobimy", "zrobiƒô", "jutro", "wkr√≥tce", "za chwilƒô", "w przysz≈Ço≈õci", "p√≥≈∫niej"]
        
        # Sprawdzanie wska≈∫nik√≥w w tek≈õcie
        for indicator in past_indicators:
            if indicator in text_lower:
                temporal_scores["przesz≈Ço≈õƒá"] += 0.15
                
        for indicator in present_indicators:
            if indicator in text_lower:
                temporal_scores["tera≈∫niejszo≈õƒá"] += 0.15
                
        for indicator in future_indicators:
            if indicator in text_lower:
                temporal_scores["przysz≈Ço≈õƒá"] += 0.15
        
        # Normalizacja wynik√≥w
        total = sum(temporal_scores.values()) or 1
        normalized = {k: round(v/total, 2) for k, v in temporal_scores.items()}
        
        # Okre≈õlenie dominujƒÖcego kontekstu czasowego
        dominant = max(normalized, key=normalized.get)
        normalized["dominujƒÖcy"] = dominant
        
        return normalized
    def extract_entities(self, text):
        """Ekstrakcja encji z tekstu (osoby, miejsca, organizacje, daty, liczby)"""
        entities = {
            "osoby": [],
            "miejsca": [],
            "organizacje": [],
            "daty": [],
            "liczby": []
        }
        
        # Proste wzorce do rozpoznawania encji
        
        # Osoby (podstawowy wzorzec imiƒô i nazwisko)
        person_pattern = re.compile(r'\b[A-Z≈öƒÜ≈π≈ª≈Å√ì≈É][a-z≈õƒá≈∫≈º≈Ç√≥≈Ñ√§√´√∂√º√ü]+ [A-Z≈öƒÜ≈π≈ª≈Å√ì≈É][a-z≈õƒá≈∫≈º≈Ç√≥≈Ñ√§√´√∂√º√ü]+\b')
        for match in person_pattern.finditer(text):
            entities["osoby"].append(match.group(0))
            
        # Miejsca (miasta, kraje)
        places = ["Warszawa", "Krak√≥w", "Wroc≈Çaw", "Pozna≈Ñ", "Gda≈Ñsk", "≈Å√≥d≈∫", "Szczecin", 
                 "Polska", "Niemcy", "Francja", "W≈Çochy", "Hiszpania", "Anglia", "USA"]
        for place in places:
            if place in text:
                entities["miejsca"].append(place)
                
        # Organizacje (proste wzorce)
        org_pattern = re.compile(r'\b(?:[A-Z≈öƒÜ≈π≈ª≈Å√ì≈É][a-z≈õƒá≈∫≈º≈Ç√≥≈Ñ√§√´√∂√º√ü]+ )?(?:[A-Z≈öƒÜ≈π≈ª≈Å√ì≈É][a-z≈õƒá≈∫≈º≈Ç√≥≈Ñ√§√´√∂√º√ü]+ )?[A-Z≈öƒÜ≈π≈ª≈Å√ì≈É][a-z≈õƒá≈∫≈º≈Ç√≥≈Ñ√§√´√∂√º√ü]* (?:sp\. z o\.o\.|S\.A\.|Inc\.|Ltd\.|GmbH)\b')
        for match in org_pattern.finditer(text):
            entities["organizacje"].append(match.group(0))
            
        # Popularne organizacje
        orgs = ["Google", "Microsoft", "Facebook", "Apple", "Amazon", "Twitter", "Netflix", "Allegro", "PKO", "PZU"]
        for org in orgs:
            if org in text:
                entities["organizacje"].append(org)
                
        # Daty
        date_patterns = [
            re.compile(r'\b\d{1,2} (?:stycznia|lutego|marca|kwietnia|maja|czerwca|lipca|sierpnia|wrze≈õnia|pa≈∫dziernika|listopada|grudnia) \d{4}\b'),
            re.compile(r'\b\d{1,2}\.\d{1,2}\.\d{2,4}\b'),
            re.compile(r'\b\d{4}-\d{1,2}-\d{1,2}\b')
        ]
        
        for pattern in date_patterns:
            for match in pattern.finditer(text):
                entities["daty"].append(match.group(0))
                
        # Liczby
        number_pattern = re.compile(r'\b\d+(?:[.,]\d+)?\b')
        for match in number_pattern.finditer(text):
            entities["liczby"].append(match.group(0))
            
        # Usuniƒôcie duplikat√≥w
        for entity_type in entities:
            entities[entity_type] = list(set(entities[entity_type]))
            
        return entities
    
    def analyze_conversation(self, messages):
        """Analiza ca≈Çej konwersacji"""
        if not messages:
            return {}
            
        # Ekstrahuj teksty z wiadomo≈õci
        texts = [msg.get("content", "") for msg in messages]
        full_text = " ".join(texts)
        
        # Analiza ca≈Çego tekstu
        overall_analysis = self.analyze_text(full_text)
        
        # ≈öledzenie trendu sentymentu
        sentiment_values = []
        for msg in messages:
            content = msg.get("content", "")
            if content:
                sentiment = self.analyze_sentiment(content)
                if sentiment["dominujƒÖcy"] == "pozytywny":
                    sentiment_values.append(sentiment["pozytywny"])
                elif sentiment["dominujƒÖcy"] == "negatywny":
                    sentiment_values.append(-sentiment["negatywny"])
                else:
                    sentiment_values.append(0)
        
        # Okre≈õlenie trendu sentymentu
        sentiment_trend = "stabilny"
        avg_sentiment = "neutralny"
        if sentiment_values:
            if len(sentiment_values) >= 3:
                first_half = sentiment_values[:len(sentiment_values)//2]
                second_half = sentiment_values[len(sentiment_values)//2:]
                avg_first = sum(first_half) / len(first_half) if first_half else 0
                avg_second = sum(second_half) / len(second_half) if second_half else 0
                
                if avg_second > avg_first + 0.2:
                    sentiment_trend = "rosnƒÖcy"
                elif avg_second < avg_first - 0.2:
                    sentiment_trend = "malejƒÖcy"
            
            avg_value = sum(sentiment_values) / len(sentiment_values)
            if avg_value > 0.2:
                avg_sentiment = "pozytywny"
            elif avg_value < -0.2:
                avg_sentiment = "negatywny"
        
        # Analiza sp√≥jno≈õci tematycznej
        topic_consistency = {"sp√≥jno≈õƒá": "wysoka", "warto≈õƒá": 0.8}
        if len(texts) >= 2:
            topics_per_message = [set(self.detect_topics(txt).keys()) for txt in texts]
            consistency_scores = []
            
            for i in range(1, len(topics_per_message)):
                current = topics_per_message[i]
                previous = topics_per_message[i-1]
                
                if current and previous:  # Je≈õli oba zestawy niepuste
                    similarity = len(current.intersection(previous)) / len(current.union(previous)) if current.union(previous) else 0
                    consistency_scores.append(similarity)
            
            if consistency_scores:
                avg_consistency = sum(consistency_scores) / len(consistency_scores)
                topic_consistency["warto≈õƒá"] = round(avg_consistency, 2)
                
                if avg_consistency < 0.3:
                    topic_consistency["sp√≥jno≈õƒá"] = "niska"
                elif avg_consistency < 0.6:
                    topic_consistency["sp√≥jno≈õƒá"] = "≈õrednia"
        
        # Analiza zmian intencji
        intention_sequence = []
        for msg in messages:
            if msg.get("role") == "user" and msg.get("content"):
                intention = self.detect_intention(msg.get("content"))
                intention_sequence.append(intention["dominujƒÖca"])
                
        intention_changes = {"zmiany": "brak", "sekwencja": intention_sequence}
        
        if len(intention_sequence) >= 3:
            changes_count = sum(1 for i in range(1, len(intention_sequence)) if intention_sequence[i] != intention_sequence[i-1])
            change_rate = changes_count / (len(intention_sequence) - 1)
            
            if change_rate > 0.7:
                intention_changes["zmiany"] = "czƒôste"
            elif change_rate > 0.3:
                intention_changes["zmiany"] = "sporadyczne"
        
        return {
            "overall_analysis": overall_analysis,
            "sentiment_trend": {
                "trend": sentiment_trend,
                "≈õredni_sentyment": avg_sentiment,
                "warto≈õci": sentiment_values
            },
            "topic_consistency": topic_consistency,
            "main_topics": list(overall_analysis["topics"].keys()),
            "intention_changes": intention_changes
        }

class SemanticIntegration:
    """Klasa integrujƒÖca analizƒô semantycznƒÖ z g≈Ç√≥wnym systemem"""
    def __init__(self, db_path=None):
        self.db_path = db_path
        self.analyzer = SemanticAnalyzer()
        
        # Inicjalizacja tabeli semantic_metadata, je≈õli nie istnieje
        if db_path:
            conn = sqlite3.connect(db_path)
            conn.execute('''
                CREATE TABLE IF NOT EXISTS semantic_metadata (
                    id TEXT PRIMARY KEY,
                    user_id TEXT,
                    message_id TEXT,
                    role TEXT,
                    topics TEXT,
                    sentiment TEXT,
                    intention TEXT,
                    entities TEXT,
                    complexity TEXT,
                    temporal_context TEXT,
                    timestamp REAL
                )
            ''')
            conn.commit()
            conn.close()
    
    def enhance_chat_response(self, user_id, query, response, message_history=None):
        """Wzbogaca odpowied≈∫ o analizƒô semantycznƒÖ"""
        # Analiza zapytania u≈ºytkownika
        query_analysis = self.analyzer.analyze_text(query)
        
        # Analiza odpowiedzi
        response_analysis = self.analyzer.analyze_text(response)
        
        # Analiza ca≈Çej konwersacji
        conversation_analysis = {}
        if message_history:
            conversation_analysis = self.analyzer.analyze_conversation(message_history)
        
        # Ekstrakcja encji
        entities = query_analysis.get("entities", {})
        
        # Generowanie rekomendacji
        recommendations = self._generate_recommendations(query_analysis, response_analysis, conversation_analysis)
        
        # Zapisanie metadanych semantycznych w bazie danych
        self.store_semantic_data(user_id, query, query_analysis)
        
        return {
            "query_analysis": query_analysis,
            "response_analysis": response_analysis,
            "conversation_analysis": conversation_analysis,
            "entities": entities,
            "recommendations": recommendations
        }
    
    def _generate_recommendations(self, query_analysis, response_analysis, conversation_analysis):
        """Generuje rekomendacje na podstawie analizy semantycznej"""
        recommendations = {
            "topics": [],
            "intentions": [],
            "tone_suggestions": [],
            "follow_up_questions": [],
            "context_needs": []
        }
        
        # Rekomendacje temat√≥w
        if query_analysis.get("topics"):
            for topic, score in sorted(query_analysis["topics"].items(), key=lambda x: x[1], reverse=True)[:3]:
                recommendations["topics"].append({"topic": topic, "score": score})
        
        # Rekomendacje intencji
        query_intention = query_analysis.get("intention", {}).get("dominujƒÖca")
        if query_intention:
            recommendations["intentions"].append(query_intention)
        
        # Rekomendacje tonu
        query_sentiment = query_analysis.get("sentiment", {}).get("dominujƒÖcy")
        if query_sentiment == "pozytywny":
            recommendations["tone_suggestions"] = ["pozytywny", "entuzjastyczny", "pomocny"]
        elif query_sentiment == "negatywny":
            recommendations["tone_suggestions"] = ["empatyczny", "profesjonalny", "rzeczowy"]
        else:
            recommendations["tone_suggestions"] = ["informacyjny", "neutralny", "rzeczowy"]
        
        # Generowanie pyta≈Ñ uzupe≈ÇniajƒÖcych
        topics = list(query_analysis.get("topics", {}).keys())
        entities = query_analysis.get("entities", {})
        keywords = query_analysis.get("keywords", [])
        
        follow_up_templates = [
            "Czy potrzebujesz bardziej szczeg√≥≈Çowych informacji na temat {topic}?",
            "Czy chcia≈Çby≈õ dowiedzieƒá siƒô wiƒôcej o {keyword}?",
            "Czy masz jakie≈õ konkretne pytania dotyczƒÖce {entity}?",
            "Czy mogƒô pom√≥c Ci w jeszcze czym≈õ zwiƒÖzanym z {topic}?"
        ]
        
        # Tworzenie pyta≈Ñ uzupe≈ÇniajƒÖcych
        if topics:
            topic = random.choice(topics)
            recommendations["follow_up_questions"].append(follow_up_templates[0].format(topic=topic))
            
        if keywords:
            keyword = random.choice(keywords)
            recommendations["follow_up_questions"].append(follow_up_templates[1].format(keyword=keyword))
            
        for entity_type, entity_list in entities.items():
            if entity_list and random.random() < 0.5:  # 50% szans na dodanie pytania o encjƒô
                entity = random.choice(entity_list)
                recommendations["follow_up_questions"].append(follow_up_templates[2].format(entity=entity))
        
        # Potrzeby kontekstowe
        if not entities.get("osoby") and ("osoba" in topics or "ludzie" in topics):
            recommendations["context_needs"].append("informacje o osobach")
            
        if not entities.get("daty") and ("czas" in topics or "harmonogram" in topics):
            recommendations["context_needs"].append("informacje o czasie/datach")
            
        # Usuniƒôcie duplikat√≥w i ograniczenie do sensownej liczby
        recommendations["follow_up_questions"] = list(set(recommendations["follow_up_questions"]))[:3]
        
        return recommendations
    
    def get_semantic_metadata_for_db(self, user_id, text, role):
        """Przygotowuje metadane semantyczne do zapisu w bazie danych"""
        analysis = self.analyzer.analyze_text(text)
        
        semantic_metadata = {
            "user_id": user_id,
            "role": role,
            "semantic_metadata": {
                "topics": analysis.get("topics", {}),
                "sentiment": analysis.get("sentiment", {}).get("dominujƒÖcy", "neutralny"),
                "intention": analysis.get("intention", {}).get("dominujƒÖca", "nieznana"),
                "entities": analysis.get("entities", {}),
                "complexity": analysis.get("complexity", {}).get("poziom", "≈õrednia"),
                "temporal_context": analysis.get("temporal_context", {}).get("dominujƒÖcy", "tera≈∫niejszo≈õƒá")
            }
        }
        
        return semantic_metadata
        
    def store_semantic_data(self, user_id, query, analysis):
        """Zapisuje metadane semantyczne w bazie danych"""
        if not self.db_path or not user_id or not query:
            return False
            
        try:
            conn = sqlite3.connect(self.db_path)
            c = conn.cursor()
            id = uuid.uuid4().hex
            message_id = uuid.uuid4().hex
            
            # Konwersja p√≥l s≈Çownikowych do JSON
            topics_json = json.dumps(analysis.get("topics", {}), ensure_ascii=False)
            sentiment = analysis.get("sentiment", {}).get("dominujƒÖcy", "neutralny")
            intention = analysis.get("intention", {}).get("dominujƒÖca", "nieznana")
            entities_json = json.dumps(analysis.get("entities", {}), ensure_ascii=False)
            complexity = analysis.get("complexity", {}).get("poziom", "≈õrednia")
            temporal_context = analysis.get("temporal_context", {}).get("dominujƒÖcy", "tera≈∫niejszo≈õƒá")
            
            c.execute("INSERT INTO semantic_metadata VALUES(?,?,?,?,?,?,?,?,?,?,?)",
                     (id, user_id, message_id, "user", topics_json, sentiment, intention, entities_json, complexity, temporal_context, time.time()))
            
            conn.commit()
            conn.close()
            return True
        except Exception as e:
            print(f"B≈ÇƒÖd podczas zapisywania danych semantycznych: {e}")
            return False

# Inicjalizacja modu≈Çu analizy semantycznej
semantic_analyzer = SemanticAnalyzer()

# Inicjalizacja modu≈Çu analizy semantycznej
semantic_analyzer = SemanticAnalyzer()
semantic_integration = SemanticIntegration(DB_PATH)
log_info("Semantic module initialized", "SEMANTIC")

# Public API
def semantic_analyze(text: str)->Dict[str,Any]:
    """Analyze text semantically"""
    return semantic_analyzer.analyze_text(text)

def semantic_analyze_conversation(messages: List[Dict[str,str]])->Dict[str,Any]:
    """Analyze entire conversation"""
    return semantic_analyzer.analyze_conversation(messages)

def semantic_enhance_response(answer: str, context: str="")->Dict[str,Any]:
    """Enhance response based on semantic analysis"""
    analysis = semantic_analyzer.analyze_text(answer)
    enhanced = answer
    sentiment = analysis.get("sentyment", {})
    
    if sentiment.get("dominujƒÖcy") == "negatywny":
        if not any(word in answer.lower() for word in ["przepraszam", "rozumiem", "przykro"]):
            enhanced = "Rozumiem, " + answer[0].lower() + answer[1:]
    
    return {"ok": True, "original": answer, "enhanced": enhanced, "analysis": analysis}


def embed_text(text: str) -> List[float]:
    """
    Generuje embedding dla tekstu u≈ºywajƒÖc dostƒôpnego providera.
    
    Args:
        text: Tekst do embeddingu
        
    Returns:
        List[float]: Wektor embedding
    """
    try:
        # U≈ºyj semantic_analyze do uzyskania embedding
        analysis = semantic_analyze(text)
        return analysis.get("embedding", [])
    except Exception as e:
        log_error(f"Text embedding failed: {e}")
        return []


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """
    Oblicza podobie≈Ñstwo kosinusowe miƒôdzy dwoma wektorami.
    
    Args:
        vec1: Pierwszy wektor
        vec2: Drugi wektor
        
    Returns:
        float: Podobie≈Ñsto w zakresie [0, 1]
    """
    try:
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0
            
        import math
        
        # Oblicz iloczyn skalarny
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        
        # Oblicz normy
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        return dot_product / (norm1 * norm2)
        
    except Exception as e:
        log_error(f"Cosine similarity calculation failed: {e}")
        return 0.0


__all__ = [
    'SemanticAnalyzer', 'SemanticIntegration',
    'semantic_analyzer', 'semantic_integration',
    'semantic_analyze', 'semantic_analyze_conversation', 'semantic_enhance_response',
    'embed_text', 'cosine_similarity'
]
