# üß† CO DZIEJE SIƒò GDY PISZESZ W CZACIE - PE≈ÅNY FLOW

## ‚ö° KROK-PO-KROKU - CO SIƒò AUTOMATYCZNIE WYWO≈ÅUJE

Gdy wpiszesz wiadomo≈õƒá typu "Cze≈õƒá!" i klikniesz Enter:

---

### 1Ô∏è‚É£ FRONTEND (Angular)
**Plik**: `frontend/src/app/components/chat/chat.component.ts`

```typescript
sendMessage() {
  // 1. Dodaje twojƒÖ wiadomo≈õƒá do UI
  this.chatService.addMessage('user', 'Cze≈õƒá!');
  
  // 2. Tworzy request
  const request = {
    messages: [{ role: 'user', content: 'Cze≈õƒá!' }],
    user_id: 'web_user',
    use_memory: true,      // ‚Üê PAMIƒòƒÜ W≈ÅƒÑCZONA!
    auto_learn: true       // ‚Üê AUTO-NAUKA W≈ÅƒÑCZONA!
  };
  
  // 3. Wysy≈Ça do backend API
  POST /api/chat/assistant
}
```

---

### 2Ô∏è‚É£ BACKEND ENDPOINT (FastAPI)
**Plik**: `assistant_endpoint.py` ‚Üí funkcja `chat_assistant()`

```python
@router.post("/assistant")
async def chat_assistant(body: ChatRequest):
    # 1. Autoryzacja (Bearer token)
    # 2. Deleguje do COGNITIVE ENGINE:
    result = await cognitive_engine.process_message(
        user_id="web_user",
        messages=[{"role":"user","content":"Cze≈õƒá!"}],
        req=request
    )
    
    # 3. Po otrzymaniu odpowiedzi ‚Üí ZAPIS DO PAMIƒòCI:
    _save_turn_to_memory("Cze≈õƒá!", result["answer"], "web_user")
    
    # 4. Je≈õli auto_learn=True ‚Üí AUTO-NAUKA:
    if body.auto_learn:
        _auto_learn_from_turn("Cze≈õƒá!", result["answer"])
    
    return result
```

---

### 3Ô∏è‚É£ COGNITIVE ENGINE (G≈Ç√≥wny m√≥zg)
**Plik**: `core/cognitive_engine.py` ‚Üí klasa `CognitiveEngine`

#### Etap A: FAST PATH CHECK
```python
# Sprawdza czy to jest proste zapytanie
# kt√≥re mo≈ºna obs≈Çu≈ºyƒá bez pe≈Çnej kognicji
for handler in FAST_PATH_HANDLERS:
    if handler.can_handle("Cze≈õƒá!"):
        return quick_response
```

**FAST PATH HANDLERS** (11 sztuk):
1. ‚úÖ `intent_health()` - "/health" ‚Üí status
2. ‚úÖ `intent_help()` - "pomoc" ‚Üí instrukcje
3. ‚úÖ `intent_memory_stats()` - "statystyki pamiƒôci"
4. ‚úÖ `intent_psyche_status()` - "stan psychiki"
5. ‚úÖ `intent_who_are_you()` - "kim jeste≈õ"
6. ‚úÖ `intent_clear_memory()` - "wyczy≈õƒá pamiƒôƒá"
7. ‚úÖ `intent_search_memory()` - "szukaj w pamiƒôci X"
8. ‚úÖ `intent_web_search()` - "szukaj w internecie X"
9. ‚úÖ `intent_time_date()` - "kt√≥ra godzina" / "jaka data"
10. ‚úÖ `intent_weather()` - "pogoda"
11. ‚úÖ `intent_calculate()` - "oblicz 2+2"

Je≈õli ≈ªADEN nie pasuje ‚Üí przechodzi dalej...

---

#### Etap B: COGNITIVE MODE DETECTION
```python
# Analizuje typ zapytania i dobiera tryb:
if "wyja≈õnij" or "dlaczego" in message:
    mode = FULL_COGNITIVE  # Pe≈Çna refleksja
elif "polityka" or "kontrowersja" in message:
    mode = MULTI_AGENT     # Wiele perspektyw
elif "przysz≈Ço≈õƒá" or "trend" in message:
    mode = PREDICTIVE      # Predykcje
else:
    mode = ENHANCED        # Standardowy
```

**DOSTƒòPNE TRYBY:**
- `BASIC` - Prosty LLM call
- `ENHANCED` - LLM + pamiƒôƒá
- `ADVANCED` - LLM + pamiƒôƒá + psyche
- `FULL_COGNITIVE` - Wszystko + refleksja
- `MULTI_AGENT` - Wiele agent√≥w (debata)
- `PREDICTIVE` - Predykcje przysz≈Ço≈õci

---

#### Etap C: PRZYGOTOWANIE KONTEKSTU
```python
# 1. Pobiera OSTATNIE 10 WIADOMO≈öCI z konwersacji
context = messages[-10:]

# 2. HIERARCHICAL MEMORY - pobiera episody:
memory_context = get_hierarchical_context(
    query="Cze≈õƒá!",
    user_id="web_user"
)
# Zwraca:
# - episodic_memories (przesz≈Çe rozmowy)
# - semantic_knowledge (fakty)
# - procedural_knowledge (jak robiƒá rzeczy)
# - working_memory (STM - kr√≥tkoterminowa)

# 3. PSYCHE STATUS:
psyche_state = {
    "mood": 0.75,      # Nastr√≥j AI
    "energy": 0.80,    # Energia
    "focus": 0.90      # Skupienie
}
```

---

#### Etap D: ADVANCED COGNITIVE ENGINE
**Plik**: `core/advanced_cognitive_engine.py`

```python
cognitive_result = await self.advanced_engine.process_message(
    user_message="Cze≈õƒá!",
    user_id="web_user",
    conversation_context=context,
    cognitive_mode=ENHANCED
)
```

**CO ROBI ADVANCED ENGINE:**

##### 1. **SYSTEM REFLEKSJI** (`core/self_reflection.py`)
```python
# Analizuje w≈Çasne odpowiedzi i uczy siƒô z nich
reflection_insights = await reflect_on_response(
    user_query="Cze≈õƒá!",
    initial_response="...",
    conversation_history=context
)
# Zwraca: [
#   {"improvement_type": "tone", "suggestion": "..."},
#   {"improvement_type": "clarity", "suggestion": "..."}
# ]
```

##### 2. **MULTI-AGENT SYSTEM** (`core/multi_agent.py`)
```python
# Tworzy wiele agent√≥w o r√≥≈ºnych perspektywach
agents = [
    {"name": "Analyst", "perspective": "analytical"},
    {"name": "Creative", "perspective": "creative"},
    {"name": "Critic", "perspective": "critical"}
]

agent_responses = []
for agent in agents:
    response = await agent.generate_response("Cze≈õƒá!")
    agent_responses.append(response)

# Syntetyzuje wszystkie perspektywy
final_answer = synthesize_perspectives(agent_responses)
```

##### 3. **INNER LANGUAGE** (`core/inner_language.py`)
```python
# Kompresuje wiedzƒô do wewnƒôtrznego jƒôzyka
inner_thought = await compress_to_inner_language(
    external_input="Cze≈õƒá!",
    context=memory_context
)
# Zwraca: {
#   "compressed_form": "GREET_NEW_USER_FRIENDLY",
#   "compression_level": 0.85,
#   "semantic_tokens": ["greeting", "friendly", "casual"]
# }
```

##### 4. **FUTURE PREDICTOR** (`core/future_predictor.py`)
```python
# Przewiduje co u≈ºytkownik mo≈ºe zapytaƒá dalej
predictions = await predict_user_next_questions(
    current_query="Cze≈õƒá!",
    user_history=context,
    user_profile=user_model
)
# Zwraca: [
#   {"query": "Jak siƒô masz?", "probability": 0.45},
#   {"query": "Co potrafisz?", "probability": 0.35},
#   {"query": "Pom√≥≈º mi z...", "probability": 0.20}
# ]
```

##### 5. **KNOWLEDGE COMPRESSION** (`core/knowledge_compression.py`)
```python
# Kompresuje wiedzƒô do wektor√≥w
compressed_knowledge = await compress_knowledge(
    raw_facts=ltm_results,
    query="Cze≈õƒá!",
    compression_ratio=0.3
)
# Zwraca: {
#   "knowledge_vectors": [...],
#   "core_concepts": ["greeting", "politeness"],
#   "research_sources": [...]
# }
```

---

#### Etap E: WEB RESEARCH (je≈õli potrzebne)
**Plik**: `core/research.py` ‚Üí funkcja `autonauka()`

```python
# Je≈õli zapytanie wymaga aktualnej wiedzy:
if needs_web_research("Cze≈õƒá!"):  # False dla "Cze≈õƒá!"
    web_result = await autonauka(
        query="Cze≈õƒá!",
        topk=8,
        user_id="web_user"
    )
    # U≈ºywa:
    # - SERPAPI (Google search)
    # - Firecrawl (scraping)
    # - Embeddings (semantic search)
    # - LTM (long-term memory storage)
```

---

#### Etap F: LLM GENERATION
**Plik**: `core/llm.py` ‚Üí funkcja `call_llm()`

```python
# Przygotowuje finalny prompt:
messages = [
    {
        "role": "system",
        "content": """Jeste≈õ Mordzix AI - zaawansowany asystent kognitywny.
        
        KONTEKST Z PAMIƒòCI:
        - Episodic: [poprzednie rozmowy]
        - Semantic: [fakty z LTM]
        - Procedural: [procedury]
        
        PSYCHE STATE:
        - Mood: 0.75
        - Energy: 0.80
        - Focus: 0.90
        
        REFLECTION INSIGHTS:
        - [insights z refleksji]
        
        PREDICTIONS:
        - U≈ºytkownik prawdopodobnie zapyta: "Jak siƒô masz?"
        """
    },
    {"role": "user", "content": "Cze≈õƒá!"}
]

# Wywo≈Çuje LLM (GLM-4.5 przez DeepInfra)
response = await llm_client.chat_completion(
    messages=messages,
    temperature=0.7,
    max_tokens=2000
)
# ‚Üí "Cze≈õƒá! Jak mogƒô Ci dzisiaj pom√≥c?"
```

---

### 4Ô∏è‚É£ POST-PROCESSING (Po wygenerowaniu odpowiedzi)

#### A. ZAPIS DO PAMIƒòCI STM (Short-Term Memory)
**Plik**: `core/memory.py` ‚Üí `_save_turn_to_memory()`

```python
# Zapisuje w bazie SQLite:
INSERT INTO stm (user_id, role, content, timestamp)
VALUES 
  ('web_user', 'user', 'Cze≈õƒá!', 1729123456.789),
  ('web_user', 'assistant', 'Cze≈õƒá! Jak mogƒô...', 1729123458.123)
```

#### B. AUTO-LEARNING (Je≈õli auto_learn=True)
**Plik**: `core/memory.py` ‚Üí `_auto_learn_from_turn()`

```python
# Analizuje rozmowƒô i wydobywa fakty:
facts_to_learn = extract_facts_from_conversation(
    user_msg="Cze≈õƒá!",
    assistant_msg="Cze≈õƒá! Jak mogƒô..."
)

# Zapisuje do LTM (Long-Term Memory):
for fact in facts_to_learn:
    ltm_save(
        fact=fact,
        confidence=0.85,
        embedding=embed(fact)
    )
```

#### C. PSYCHE UPDATE
**Plik**: `core/memory.py` ‚Üí `psy_observe_text()`

```python
# Analizuje tekst u≈ºytkownika pod kƒÖtem emocji:
sentiment = analyze_sentiment("Cze≈õƒá!")  # ‚Üí 0.7 (pozytywny)

# Aktualizuje stan psychiczny AI:
UPDATE psyche_state SET
  mood = mood * 0.9 + sentiment * 0.1,
  energy = energy - 0.01,
  focus = focus + 0.02
WHERE id = 1
```

#### D. USER MODEL UPDATE
**Plik**: `core/user_model.py` ‚Üí `update_user_model()`

```python
# Aktualizuje profil u≈ºytkownika:
user_model.update({
    "last_interaction": "2025-10-16T15:30:45",
    "interaction_count": +1,
    "preferred_tone": "friendly",
    "topics_of_interest": ["general_chat"],
    "engagement_level": 0.8
})
```

---

### 5Ô∏è‚É£ RESPONSE DO FRONTENDU

```json
{
  "ok": true,
  "answer": "Cze≈õƒá! Jak mogƒô Ci dzisiaj pom√≥c?",
  "sources": [],
  "metadata": {
    "source": "advanced_cognitive_engine",
    "cognitive_mode": "enhanced",
    "processing_time": 1.234,
    "confidence_score": 0.95,
    "originality_score": 0.75,
    "reflection_insights_count": 2,
    "agent_perspectives_count": 0,
    "future_predictions_count": 3,
    "memory_context_used": true,
    "hierarchical_memory": true,
    "predicted_follow_ups": [
      "Jak siƒô masz?",
      "Co potrafisz?",
      "Pom√≥≈º mi..."
    ]
  }
}
```

---

## üìä PODSUMOWANIE - CO SIƒò DZIEJE AUTOMATYCZNIE:

### ‚úÖ ZAWSZE WYWO≈ÅYWANE (dla ka≈ºdej wiadomo≈õci):

1. **Authorization** - Bearer token check
2. **Cognitive Engine** - g≈Ç√≥wny orchestrator
3. **Fast Path Check** - 11 intent handlers
4. **Cognitive Mode Detection** - wyb√≥r trybu przetwarzania
5. **Memory Context** - pobranie kontekstu z pamiƒôci:
   - Hierarchical Memory (episodic, semantic, procedural)
   - STM (short-term - ostatnie 10 wiadomo≈õci)
   - LTM (long-term - fakty)
6. **Psyche Status** - stan emocjonalny AI
7. **LLM Generation** - wywo≈Çanie modelu GLM-4.5
8. **STM Save** - zapis rozmowy do bazy
9. **Psyche Update** - aktualizacja stanu
10. **User Model Update** - aktualizacja profilu u≈ºytkownika

### üîß OPCJONALNIE WYWO≈ÅYWANE (zale≈ºy od trybu/zapytania):

11. **Self-Reflection** - analiza w≈Çasnych odpowiedzi
12. **Multi-Agent** - wiele perspektyw (dla kontrowersyjnych temat√≥w)
13. **Inner Language** - kompresja semantyczna
14. **Future Predictor** - przewidywanie nastƒôpnych pyta≈Ñ
15. **Knowledge Compression** - kompresja wiedzy do wektor√≥w
16. **Web Research (autonauka)** - tylko je≈õli zapytanie wymaga internetu:
    - SERPAPI search
    - Firecrawl scraping
    - Embeddings
    - LTM save
17. **Auto-Learning** - ekstrakcja fakt√≥w i zapis do LTM

---

## üéØ LICZBY:

- **Minimum system√≥w wywo≈Çanych**: 10
- **Maximum system√≥w wywo≈Çanych**: 17
- **Liczba baz danych**: 1 (SQLite z 6 tabelami)
- **Liczba modeli AI**: 2 (GLM-4.5 + embeddings)
- **Liczba external API**: do 4 (SERPAPI, Firecrawl, Stability, ElevenLabs)
- **≈öredni czas przetwarzania**: 1-3 sekundy

---

## üìÅ KLUCZOWE PLIKI:

```
assistant_endpoint.py          ‚Üê G≈Ç√≥wny endpoint
core/cognitive_engine.py       ‚Üê Orchestrator
core/advanced_cognitive_engine.py  ‚Üê 5 system√≥w kognitywnych
core/intent_dispatcher.py      ‚Üê 11 fast path handlers
core/memory.py                 ‚Üê STM/LTM/Psyche
core/hierarchical_memory.py    ‚Üê Hierarchical memory
core/self_reflection.py        ‚Üê Refleksja
core/multi_agent.py            ‚Üê Multi-agent
core/inner_language.py         ‚Üê Inner language
core/future_predictor.py       ‚Üê Predykcje
core/knowledge_compression.py  ‚Üê Kompresja wiedzy
core/research.py               ‚Üê autonauka (web)
core/llm.py                    ‚Üê GLM-4.5 client
core/user_model.py             ‚Üê Profil u≈ºytkownika
```

---

**TO WSZYSTKO DZIEJE SIƒò AUTOMATYCZNIE GDY WPISZESZ "CZE≈öƒÜ!"** üöÄ
